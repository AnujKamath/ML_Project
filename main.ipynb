{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plot_mfcc(audio, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfccs_delta = librosa.feature.delta(mfccs)  \n",
    "    mfccs_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    librosa.display.specshow(mfccs, x_axis='time', sr=sr, cmap='coolwarm')\n",
    "    plt.colorbar(label='MFCCs')\n",
    "    plt.title('MFCCs')\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    librosa.display.specshow(mfccs_delta, x_axis='time', sr=sr, cmap='coolwarm')\n",
    "    plt.colorbar(label='Delta MFCCs')\n",
    "    plt.title('Delta MFCCs')\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    librosa.display.specshow(mfccs_delta2, x_axis='time', sr=sr, cmap='coolwarm')\n",
    "    plt.colorbar(label='Delta-Delta MFCCs')\n",
    "    plt.title('Delta-Delta MFCCs')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_kmeans(audio_path, k=2):\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    speakers = []\n",
    "    paths = []\n",
    "    \n",
    "    mfccs = kmeans_plot_mfcc(audio, sr)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    mfccs = mfccs.T \n",
    "    mfccs_scaled = scaler.fit_transform(mfccs)\n",
    "    \n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k+1, random_state=42)\n",
    "    labels = kmeans.fit_predict(mfccs_scaled)\n",
    "    for cluster in range(k+1):\n",
    "        cluster_indices = np.where(labels == cluster)[0]\n",
    "        cluster_audio = []\n",
    "        hop_length = 512\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            start_sample = idx * hop_length\n",
    "            end_sample = (idx + 1) * hop_length\n",
    "            cluster_audio.append(audio[start_sample:end_sample])\n",
    "        \n",
    "        cluster_audio = np.concatenate(cluster_audio)\n",
    "        \n",
    "        output_file = f'kmeans/cluster_{cluster}.wav'\n",
    "        sf.write(output_file, cluster_audio, sr)\n",
    "        paths.append(os.path.abspath(output_file))\n",
    "        speakers.append((cluster_audio, sr))\n",
    "        print(f'Cluster {cluster} audio saved to {output_file}')\n",
    "    \n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio, window_size=2048, hop_length=1024, sample_rate=16000):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13, n_fft=window_size, hop_length=hop_length)\n",
    "\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sample_rate)\n",
    "    pitch = [np.max(p) if np.max(p) > 0 else 0 for p in pitches.T]\n",
    "\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate, n_fft=window_size, hop_length=hop_length)\n",
    "\n",
    "    pitch_resampled = np.interp(np.linspace(0, len(pitch) - 1, spectral_contrast.shape[1]), np.arange(len(pitch)), pitch)\n",
    "\n",
    "    features = np.vstack((mfccs, spectral_contrast, pitch_resampled)).T\n",
    "    return features\n",
    "\n",
    "\n",
    "def diarize_hierarchical(audio_path, k=2, window_size=2048, hop_length=1024):\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    features = extract_audio_features(audio, window_size, hop_length, sr)\n",
    "\n",
    "    features = StandardScaler().fit_transform(features)\n",
    "\n",
    "    distance_matrix = pdist(features, metric='euclidean')\n",
    "    Z = linkage(distance_matrix, method='ward')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(Z, truncate_mode='level', p=5)\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.show()\n",
    "\n",
    "    clusters = fcluster(Z, t=k+1, criterion='maxclust')\n",
    "    print(\"Cluster labels:\", clusters)\n",
    "    unique_labels, counts = np.unique(clusters, return_counts=True)\n",
    "\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"Cluster {label}: {count} samples\")\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
    "    plt.title('PCA of Clusters')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "    centroids = np.array([features[clusters == i].mean(axis=0) for i in np.unique(clusters)])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
    "    plt.scatter(pca.transform(centroids)[:, 0], pca.transform(centroids)[:, 1], marker='X', color='red', s=100, label='Centroids')\n",
    "    plt.title('PCA of Clusters with Centroids')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    speakers = []\n",
    "    paths = []\n",
    "    for label in unique_labels:\n",
    "        cluster_indices = np.where(clusters == label)[0]\n",
    "        cluster_audio = np.concatenate([audio[i:i+1] for i in cluster_indices], axis=0)\n",
    "        output_path = f'hierarchical_clustering/cluster_{label}.wav'\n",
    "        sf.write(output_path, cluster_audio, sr)\n",
    "        print(f\"Cluster {label} audio saved to {output_path}\")\n",
    "        paths.append(os.path.abspath(output_path))\n",
    "        speakers.append((cluster_audio, sr))\n",
    "\n",
    "    features_df = pd.DataFrame(features)\n",
    "    features_df['Cluster'] = clusters\n",
    "    features_df_sorted = features_df.sort_values(by='Cluster')\n",
    "\n",
    "    heatmap_data = features_df_sorted.drop(columns='Cluster')\n",
    "    palette = sns.color_palette(\"coolwarm\", n_colors=len(np.unique(clusters)))\n",
    "    row_colors = features_df_sorted['Cluster'].map({i: palette[i] for i in range(len(palette))})\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    heatmap_data = pd.DataFrame(scaler.fit_transform(heatmap_data), columns=heatmap_data.columns)\n",
    "\n",
    "    sns.clustermap(\n",
    "        heatmap_data.T,\n",
    "        cmap='coolwarm',\n",
    "        linewidths=0.5,\n",
    "        row_colors=row_colors,\n",
    "        yticklabels=False,\n",
    "        annot=True, \n",
    "        fmt=\".2f\"    \n",
    "    )\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.show()\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_gmm(audio_path, k=2):\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    speakers = []\n",
    "    paths = []\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13).T   \n",
    "    scaler = StandardScaler()\n",
    "    mfccs_scaled = scaler.fit_transform(mfccs)\n",
    "    gmm = GaussianMixture(n_components=k+1, random_state=42, warm_start=False)\n",
    "    gmm.fit(mfccs_scaled)\n",
    "\n",
    "    labels = gmm.predict(mfccs_scaled)\n",
    "    \n",
    "    for cluster in range(k+1):\n",
    "        cluster_indices = np.where(labels == cluster)[0]\n",
    "        cluster_audio = []\n",
    "        hop_length = 512\n",
    "        \n",
    "        for idx in cluster_indices:\n",
    "            start_sample = idx * hop_length\n",
    "            end_sample = (idx + 1) * hop_length\n",
    "            cluster_audio.append(audio[start_sample:end_sample])\n",
    "        \n",
    "        cluster_audio = np.concatenate(cluster_audio)\n",
    "        \n",
    "        output_file = f'gmm/cluster_{cluster}.wav'\n",
    "        sf.write(output_file, cluster_audio, sr)\n",
    "        speakers.append((cluster_audio, sr))\n",
    "        paths.append(os.path.abspath(output_file))\n",
    "        print(f'Cluster {cluster} audio saved to {output_file}')\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "# notebook_login()\n",
    "\n",
    "login('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')\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(audio, sr):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest():\n",
    "    hindi_dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"hi\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "    english_dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"en\", split=\"train\", streaming=True, trust_remote_code=True)\n",
    "    \n",
    "    hindi_iter = iter(hindi_dataset)\n",
    "    hindi_files = [next(hindi_iter) for _ in range(100)]\n",
    "\n",
    "    eng_iter = iter(english_dataset)\n",
    "    eng_files = [next(eng_iter) for _ in range(100)]\n",
    "    \n",
    "    combined_samples = hindi_files + eng_files\n",
    "    random.shuffle(combined_samples)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for sample in combined_samples:\n",
    "        features = extract_mfcc(sample['audio']['array'], sample['audio']['sampling_rate'])\n",
    "        X.append(features)\n",
    "        y.append(sample['locale'])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    \n",
    "    \n",
    "    joblib.dump(clf, 'models/random_forest_hi_en.joblib')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=['en', 'hi'])\n",
    "\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['English', 'Hindi'], yticklabels=['English', 'Hindi'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_random_forest(audio_path, retrain=False):\n",
    "    \n",
    "    if retrain:\n",
    "        train_random_forest()\n",
    "\n",
    "    clf = joblib.load('models/random_forest_hi_en.joblib')\n",
    "\n",
    "    audio, sr = librosa.load(audio_path)\n",
    "    mfcc = extract_mfcc(audio, sr)\n",
    "    lang = clf.predict([mfcc])\n",
    "    \n",
    "    return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assembling the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_drop_choices = {\n",
    "    'KMeans Clustering': diarize_kmeans,\n",
    "    'Hierarchical Clustering': diarize_hierarchical,\n",
    "    'Gaussian Mixture Model Based Clustering': diarize_gmm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize(audio_path, num_speakers, method):\n",
    "    return cluster_drop_choices[method](audio_path, num_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(audio_path, retrain_bool):\n",
    "    if retrain_bool == 'True':\n",
    "        retrain=True\n",
    "    else:\n",
    "        retrain=False\n",
    "    return classify_random_forest(audio_path, retrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown('# Speaker Diarization and Language Classification')\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_path = gr.Audio(type='filepath', label='Audio File')\n",
    "        \n",
    "    gr.Markdown('## Diarization')\n",
    "    with gr.Row():\n",
    "        method = gr.Dropdown(choices=cluster_drop_choices.keys(), label='Method of Diarization')\n",
    "        num_speakers = gr.Dropdown(choices=range(1, 11), value=2, label='Number of Speakers')\n",
    "        \n",
    "    diarize_btn = gr.Button('Diarize')\n",
    "    \n",
    "    diarized_audio_outputs = [gr.Audio(label=f'Segment {i+1}', type='filepath', visible=False) for i in range(11)]\n",
    "\n",
    "    diarize_btn.click(\n",
    "        inputs=[num_speakers],\n",
    "        fn=lambda num_speakers: [gr.update(visible=True)] * (num_speakers+1) + [gr.update(visible=False)] * (11 - num_speakers),\n",
    "        outputs=diarized_audio_outputs\n",
    "    )\n",
    "    \n",
    "    diarize_btn.click(\n",
    "        inputs=[audio_path, num_speakers, method],\n",
    "        fn=diarize,\n",
    "        outputs=diarized_audio_outputs[:(num_speakers.value+1)]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown('-------')\n",
    "    gr.Markdown('## Language Classification')\n",
    "    \n",
    "    retrain_bool = gr.Dropdown(choices=['True', 'False'], value='False', label='Retrain?')\n",
    "    classify_btn = gr.Button('Classify Language')\n",
    "    \n",
    "    classification_result_output = gr.Textbox(label='Classification Result', interactive=False, visible=False)\n",
    "\n",
    "    classify_btn.click(\n",
    "        fn=lambda: gr.update(visible=True),\n",
    "        outputs=classification_result_output\n",
    "    )\n",
    "    classify_btn.click(\n",
    "        inputs=[audio_path, retrain_bool],\n",
    "        fn=classify,\n",
    "        outputs=classification_result_output\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
